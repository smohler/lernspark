{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565ddfb9-c477-499e-8fca-f3693f89d93e",
   "metadata": {},
   "source": [
    "# Lernspark Pipeline Playground\n",
    "\n",
    "This is a jupyter notebook play ground that will allow you to discover differnet types fo SQL commands and basic data engineering and analysis type operations on a sample of a much larger set of data. This is a common day to day task in data engineering analysis. \n",
    "\n",
    "## Prereqs\n",
    "You should only be looking at this notebook after running the command `lernspark-data` and `lernspark-play` this is because this notebook is dependent on some set up to create your example data set zipped up in a `tar.gz` format. **You must run lernspark-data before lernspark-play**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdef3af4-c88d-49e4-ab49-c0cdb56bfc9a",
   "metadata": {},
   "source": [
    "# Part 0: Python Imports\n",
    "\n",
    "For this notebook depending on what other packages you may include you should keep adding `import` statements into this block. It is reocmmended to group all imports into a single cell at the top of a notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d46f05a-111f-4e42-b061-8669a522600f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: pyarrow in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (16.1.0)\n",
      "Requirement already satisfied: fastparquet in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (2024.5.0)\n",
      "Requirement already satisfied: pyspark in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (3.5.1)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from fastparquet) (2.8.3)\n",
      "Requirement already satisfied: fsspec in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from fastparquet) (2024.6.0)\n",
      "Requirement already satisfied: packaging in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from fastparquet) (24.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: six>=1.5 in /opt/homebrew/Cellar/jupyterlab/4.1.6_1/libexec/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas pyarrow fastparquet pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cbda6ba-f7c9-41a8-94a8-42cdf2a58582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tarfile\n",
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bedba34-0d0e-4774-8d52-a9d242e01f03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Part 1: Load Data into Memory\n",
    "\n",
    "The first step will be to extract the data which is stored in `~\\Downloads\\examples.tar.gz` and load it into this notebooks memory. We will extract into the system tmp folder (a common operation) and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "212fd408-969d-42c3-b671-95f77d5b9e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction completed.\n",
      "Extracted files are located in: /var/folders/b4/f3v6ww_s0_zcm_srr9ndn7jr0000gn/T/tmpcyc0ud4e\n",
      "Extracted files:\n",
      "\tBowling_teams.parquet\n",
      "\tFootball_teams.parquet\n",
      "\tBaseball_teams.parquet\n"
     ]
    }
   ],
   "source": [
    "# Get the path to the downloads folder\n",
    "downloads_folder = os.path.expanduser(\"~/Downloads\")\n",
    "\n",
    "# Specify the filename of the tar.gz file\n",
    "filename = \"examples.tar.gz\"\n",
    "\n",
    "# Construct the full path to the tar.gz file\n",
    "file_path = os.path.join(downloads_folder, filename)\n",
    "\n",
    "# Create a temporary directory\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "\n",
    "try:\n",
    "    # Open the tar.gz file\n",
    "    with tarfile.open(file_path, \"r:gz\") as tar:\n",
    "        # Extract all files to the temporary directory\n",
    "        tar.extractall(path=temp_dir)\n",
    "    print(\"Extraction completed.\")\n",
    "    print(f\"Extracted files are located in: {temp_dir}\")\n",
    "    \n",
    "    # Get the list of files in the temporary directory\n",
    "    extracted_files = os.listdir(temp_dir)\n",
    "    \n",
    "    # Print the names of the extracted files\n",
    "    print(\"Extracted files:\")\n",
    "    for file_name in extracted_files:\n",
    "        print(f\"\\t{file_name}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"I can't find {file_path}, you need to run `lernspark-play` to create you data sample zip file\")\n",
    "except tarfile.ReadError:\n",
    "    print(f\"Error reading the tar.gz file: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b486d89-0b4f-49bc-afe7-440058496669",
   "metadata": {},
   "source": [
    "## Load Parquet file into Python Memory\n",
    "Now that we have unzipped the example data we can read it into python using the parquet modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d4d1e6d-c145-4444-b643-b004dc220ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the data:\n",
      "   ID                Name  Goals  Rank        Born\n",
      "0   0     Edmond Schinner     39    79  1970-01-20\n",
      "1  84       Twila DuBuque    100    23  1970-03-17\n",
      "2  38  Vida Runolfsdottir     93    53  1970-01-15\n",
      "3  45        Nathan Hyatt     65    45  1970-04-07\n",
      "4  65      Green Johnston     79    62  1970-03-26\n",
      "\n",
      "Summary statistics:\n",
      "                 ID         Goals          Rank\n",
      "count  37706.000000  37706.000000  37706.000000\n",
      "mean      50.301225     49.992229     49.980109\n",
      "std       29.104677     29.161905     29.113241\n",
      "min        0.000000      0.000000      0.000000\n",
      "25%       25.000000     25.000000     25.000000\n",
      "50%       51.000000     50.000000     50.000000\n",
      "75%       75.000000     75.000000     75.000000\n",
      "max      100.000000    100.000000    100.000000\n"
     ]
    }
   ],
   "source": [
    "# Pick a file name you want expect printed from above\n",
    "# :: EDIT THIS LINE \n",
    "data_file = \"Football_teams.parquet\"\n",
    "table_name = 'Football_teams'\n",
    "\n",
    "# Specify the path to the extracted Parquet file\n",
    "parquet_file = os.path.join(temp_dir, data_file)\n",
    "\n",
    "try:\n",
    "    # Read the Parquet file into a DataFrame\n",
    "    df = pd.read_parquet(parquet_file)\n",
    "    \n",
    "    # Print the first few rows of the DataFrame\n",
    "    print(\"First few rows of the data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    # Print the summary statistics of the DataFrame\n",
    "    print(\"\\nSummary statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Explore the data further as needed\n",
    "    # ...\n",
    "except FileNotFoundError:\n",
    "    print(f\"Parquet file not found: {parquet_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Parquet file: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc87903-fd52-419b-a617-97d2505a5d9b",
   "metadata": {},
   "source": [
    "# Part 2: Explore Data with `pandas`\n",
    "Now in this notebook you can craft any sort of data map-reduce operations you'd like on the data. Try building some operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd274a1-27d3-44e5-bc51-bf2939e15218",
   "metadata": {},
   "source": [
    "# Part 3: Explore Data with `pyspark`\n",
    "\n",
    "There are other data analysis tool instead of pandas and some are better suited for big data such as pyspark which is a python interface for Apache Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d97e9-c483-4679-909a-1307e8a33526",
   "metadata": {},
   "source": [
    "# Part 4: Refactor Work into SQL Queries\n",
    "Now that you have explored the data multiple different ways you can solidify the map-reduce operations into a series of SQL queries that we can deploy in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dd31b48-3bf9-40eb-9e06-c74c556e0e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                Name  Goals  Rank        Born\n",
      "0    0     Edmond Schinner     39    79  1970-01-20\n",
      "1   84       Twila DuBuque    100    23  1970-03-17\n",
      "2   38  Vida Runolfsdottir     93    53  1970-01-15\n",
      "3   45        Nathan Hyatt     65    45  1970-04-07\n",
      "4   65      Green Johnston     79    62  1970-03-26\n",
      "5   14     Kattie Luettgen     13    20  1970-01-20\n",
      "6  100        Brenda Davis     23    16  1970-04-07\n",
      "7   39    Valentine Renner     83    11  1970-02-20\n",
      "8   47      Stephen Harber     19    92  1970-02-01\n",
      "9   59    Guiseppe Douglas     12    79  1970-03-31\n"
     ]
    }
   ],
   "source": [
    "# Connect to an in-memory SQLite database\n",
    "conn = sqlite3.connect(':memory:')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load the DataFrame into the SQLite database\n",
    "df.to_sql(table_name, conn, index=False, if_exists='replace')\n",
    "\n",
    "# Execute SQL queries\n",
    "query = f\"SELECT * FROM {table_name} LIMIT 10;\"\n",
    "result = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Display the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74f1b7d-e437-420d-9885-ef6b4a41f1e9",
   "metadata": {},
   "source": [
    "# Part N: Clean-up\n",
    "After we are done with out analysis we need to clean up our disk memory that we have created. While computers have lots and lots of memory today. This practice is good to keep as you never know what your application will be run on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1aef779-b88e-4d03-a744-547d59009b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary directory /var/folders/b4/f3v6ww_s0_zcm_srr9ndn7jr0000gn/T/tmp33f5n0es cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Clean up the extracted files\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"Temporary directory {temp_dir} cleaned up.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
